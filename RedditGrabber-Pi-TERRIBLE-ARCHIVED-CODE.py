<div data-tbt="1" style="display: inline;">import config # used to get the secret sensitive info needed for our APIs - not uploaded to github for security purposes<br>import praw # used for the reddit api - this is 100% needed for this code to work<br>import requests # needed to get image file size before we download images (to make sure we don't download images too large that we can't upload elsewhere).<br>import os # needed to get the file paths<br>import random #needed to pick a random subreddit to grab data from. In theory you don't have to pick a random one, you could do all at once or just one, either or.<br>from googleapiclient.discovery import build # python.exe -m pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib<br>from google.oauth2 import service_account # this and the above package are for the spreadsheet usage -- the pip command is a pain so I pasted it above.<br>from PIL import Image # for image hashing<br>import imagehash # also for image hashing<br>import re # used to split strings into a list of words later down the program<br>import pytesseract # used for optical recognition<br>import cv2 # used for parsing data and converting images before putting into tesseract OCR<br><br>#establishing our creds to gain access to the API before we start doing any calls.<br>reddit = praw.Reddit(<br>    client_id=config.config_stuff2['client_id'],<br>    client_secret=config.config_stuff2['client_secret'],<br>    user_agent=config.config_stuff2['user_agent'],<br>)<br><br># points to the keys json file that holds the dictionary of the info we need.<br>SERVICE_ACCOUNT_FILE = '/home/pi/Documents/Programming-Projects/Meme-Bot/keys.json'<br><br># website to send the oauth info to gain access to our data<br>SCOPES = ['https://www.googleapis.com/auth/spreadsheets']<br><br># Writes this variable to no value before overwriting it with the info we need, basically cleaning and prepping it<br>creds = None<br><br># Writes the creds value with the value from the keys json file above<br>creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)<br><br><br># builds a package with all the above info and version we need and the right service we need<br>service = build('sheets', 'v4', credentials=creds)<br><br><br># Call the Sheets API<br>sheet = service.spreadsheets()<br><br># to specify this variable as all of the FB Poster spreadsheet<br>result_fb = sheet.values().get(spreadsheetId=config.config_stuff4['SAMPLE_SPREADSHEET_ID'],<br>                            range="FB-Poster-Log!A:H").execute()<br><br># Get values from spreadsheet<br>values_fb = result_fb.get('values', [])<br><br># to specify this variable as all of the reddit grabber spreadsheet<br>result_rg = sheet.values().get(spreadsheetId=config.config_stuff4['SAMPLE_SPREADSHEET_ID'],<br>                            range="Reddit-Grabber-Log!A:F").execute()<br><br># Get values from spreadsheet<br>values_rg = result_rg.get('values', [])<br><br>#list of subreddits to grab memes from<br>subreddit_list = ["memes", "dankmemes", "shitposting", "Unexpected", "Wholesomememes", "me_irl", "meme",<br>                  "Memes_Of_The_Dank", "starterpacks", "animemes", "funny"]<br><br>#list of bad words / topics to avoid in our posts<br>bad_topics = ["faggot", "femboy", "nigger", "fat", "skinny", "horny", "masturbate", "anal", "sex",<br>              "racist", "homophobic", "rape", "rapist", "BDSM", "dom", "fucked", "hentai",<br>              "Joe Biden", "Biden", "Trump", "Donald Trump", "disease", "symptom", "Parkinson", "Alzhemier", "memeory loss",<br>              "COVID", "covid-19", "blocked", "bacteria", "Pandemic", "quarantine", "NATO", "Ukraine", "Russia", "Putin", "fatal",<br>              "lethal", "no cure", "cock", "pussy", "dick", "vagina", "penis", "reddit",<br>              "u/", "/r/", "feminists", "qanon", "shooting", "Uvalde",]<br><br>#picks a random subreddit from the above list<br>subreddit = reddit.subreddit(random.choice(subreddit_list)).top(time_filter="day", limit=None)<br><br><br>#flatten the list of lists returned from the fb poster spreadsheet<br>flatlist_fb = [item for items in values_fb for item in items]<br><br>#flatten the list of lists returned from the reddit grabber spreadsheet<br>flatlist_rg =[item for items in values_rg for item in items]<br><br># Initializes count<br>count = 0<br><br># For loop which contains variables and parameters necessary for grabbing the type of data we want from reddit<br>for submission in subreddit:<br><br>    # Makes sure that the url we got from the api is a string variable<br>    url = str(submission.url)<br>            <br>    # 1. make sure the post is an image<br>    # 2. make sure we don't grab the same reddit post twice -- dont need to hash images that we've already done before<br>    # 3. make sure the post is not flagged as a spoiler or NSFW<br>    # 4. make sure no bad words are in the submission post titles.<br>    if url.endswith("jpg") or url.endswith("jpeg") or url.endswith("png") and if submission.id not in flatlist_rg and if not submission.spoiler and not submission.over_18 and if not any(x in submission.title for x in bad_topics):<br><br>        # defines R variable as grabbing data from our selected url<br>        r = requests.get(url)<br><br>        # divides file size by 1000 so we can get how many kilobytes it is<br>        length = float(r.headers.get('content-length')) / 1000<br><br>        # if it is less than 4 MB or 4000 KB (alternatively for cleaner numbers you can divide by 1,000,000 and do < 4 but eh)<br>        if float(length) < 4000:<br><br>            # download the image from the "url" variable link using requests function<br>            open("image.jpg", 'wb').write(r.content)<br><br>            # hash the image we just saved<br>            hash = imagehash.dhash(Image.open("image.jpg"))<br><br>            # define all of our variables as strings to be used later<br>            submission_title_string = str(submission.title)<br>            submission_id_string = str(submission.id)<br>            submission_permalink_string = (str("https://www.reddit.com") + str(submission.permalink))<br>            submission_length_string = str(length)<br>            submission_hash_string = str(hash)<br><br>            #check to make sure the hash of the image we just tested is in the spreadsheet. False means that it's not which means it's not a duplicate image (which is good).<br>            check_hash_fb = submission_hash_string in flatlist_fb<br><br>            #check to make sure the hash of the image we just tested is in the reddit grabber spreadsheet. (We want false values only here).<br>            check_hash_rg = submission_hash_string in flatlist_rg<br><br>            # make sure the hash is not in the FB post sheet or the RG sheet already<br>            if not check_hash_fb and if not check_hash_rg:<br><br>                ##run OCR<br>                #point to where the tesseract files are in our directory<br>                pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'<br><br>                #read BGR values from image<br>                img=cv2.imread('image.jpg')<br><br>                #convert BGR values to RGB values<br>                img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br><br>                #give us the resulting text (strings) from the image<br>                ocr_result = pytesseract.image_to_string(img)<br><br>                # remove the image we just saved (since we don't actually need the file after hashing it)<br>                os.remove("image.jpg")  <br><br>                #this function converts the text from OCR into a list of individual strings, where each string is an element in a list<br>                def Convert(string):<br>                    li = list(string.split(" "))<br>                    return li<br>                list_text = Convert(ocr_result)<br><br>                # this section cleans up the list to remove the "\n' from each string in the newly created list<br>                replaced_list = []<br>                for strings in list_text:<br>                    replaced_list.append(strings.replace("\n", ""))<br><br>                #check to see if within the meme itself if there are bad words in the list above<br>                check_ocr_bad_topics = [word for word in replaced_list if word in bad_topics]<br><br>                # if no matches of bad topics in the ocr text, then proceed. But if so, try a new image.<br>                if not check_ocr_bad_topics:<br><br>                    # create an empty list to store data<br>                    Spreadsheet_Values_Append = []<br><br>                    #append list with data from variables above<br>                    Spreadsheet_Values_Append.append([submission_title_string, submission_id_string, submission_permalink_string, <br>                                                      url, submission_length_string, submission_hash_string])<br><br>                    # print what is going to be uploaded to the spreadsheet, primarily just use this for testing purposes but leave it on anyways<br>                    print(Spreadsheet_Values_Append)<br><br>                    # make a request to sheets API with the 2d list we just made to append to the bottom of the spreadsheet as a new row. <br>                    request = sheet.values().append(<br>                        spreadsheetId=config.config_stuff4['SAMPLE_SPREADSHEET_ID'],<br>                        range="Reddit-Grabber-Log!A:F", valueInputOption="USER_ENTERED",<br>                        body={"values": Spreadsheet_Values_Append}).execute()<br><br>                    # again mostly used for testing and troubleshooting but leave it on anyways for the sake of human sanity<br>                    print("Post logged to Reddit Grabber Spreadsheet")<br><br>                    #increases the count to break the loop<br>                    count += 1<br><br>                    #if the count reaches 4 (meaning it has collected 4 posts) from the randomly chosen subreddit, then we're good, if not keep going until we get 4.<br>                    if count == 4:<br><br>                        # break means to break out of the loop, meaning we're good<br>                        break <br><br>                    else:<br><br>                        # continue in this instance means to start the loop over (meaning go get another post that meet the criteria<br>                        continue <br><br>                # if this part isn't included, it will run through 4 iterations, but may not end up grabbing any posts out of the 4 attempts that meet the criteria.<br>                else:<br><br>                    # continue in this case meaning, if the post we grabbed doesn't meet the criteria, keep trying until we get one, then start increasing the count.<br>                    continue <br><br><br># just for my own sanity, to make sure we completed the whole loop and script. THe proverbial "The end." lol<br>print("\nAll posts have been logged to the spreadsheet accordingly.")<br></div>
